{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from pathlib import Path\n",
    "from latentis import PROJECT_ROOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_dir: Path = PROJECT_ROOT / \"results\" / \"exp1\"\n",
    "exp_dir.exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiments = list(exp_dir.glob(\"*\"))\n",
    "len([exp.name for exp in experiments])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from residual.nn.model_registry import model_names\n",
    "\n",
    "# plt.rcParams.update(bundles.tmlr2023())\n",
    "cmap = \"RdBu_r\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\"openclip_l\", \"clip_l\", \"dinov2_l\", \"vit_l\", \"blip_l_flickr\"]\n",
    "\n",
    "twonn_id = torch.zeros(len(models), 24, 16)\n",
    "pca_id = torch.zeros(len(models), 24, 16)\n",
    "evr_std = torch.zeros(len(models), 24, 16)\n",
    "\n",
    "pca_threshold = 0.99\n",
    "for exp_path in experiments:\n",
    "    exp_data = torch.load(exp_path, map_location=\"cpu\", weights_only=False)\n",
    "    if exp_data[\"dataset_name\"] != \"imagenet\" or exp_data[\"encoder_name\"] not in models:\n",
    "        continue\n",
    "    for (layer_idx, head_idx, _unit_type), unit_data in exp_data[\"data\"].items():\n",
    "        model_idx = models.index(exp_data[\"encoder_name\"])\n",
    "        pca_id[model_idx, layer_idx, head_idx] = (\n",
    "            torch.where(unit_data[\"pca_evr\"] > pca_threshold)[0][0] + 1\n",
    "        )\n",
    "        twonn_id[model_idx, layer_idx, head_idx] = unit_data[\"id_twonn\"]\n",
    "        evr_std[model_idx, layer_idx, head_idx] = unit_data[\"pca_evr\"][:64].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tueplots import figsizes\n",
    "\n",
    "# plt.rcParams.update(figsizes.tmlr2023(ncols=1, nrows=1))\n",
    "fig, ax = plt.subplots(nrows=1, ncols=len(models) - 1, figsize=(8, 4))\n",
    "for m, model in enumerate(models):\n",
    "    pca_mean = pca_id[m].mean(dim=1, keepdim=True)\n",
    "    std_mean = evr_std[m].mean(dim=1, keepdim=True)\n",
    "    print(pca_mean.shape)\n",
    "    twonn_mean = twonn_id[m].mean(dim=1, keepdim=True)\n",
    "    normalized_twonn_mean = twonn_mean / twonn_mean.max()\n",
    "    normalized_pca_mean = pca_mean / pca_mean.max()\n",
    "    normalized_std_mean = std_mean / std_mean.max()\n",
    "    ratio_mean = (pca_id[m] / twonn_id[m]).mean(dim=1, keepdim=True)\n",
    "    normalized_ratio_mean = ratio_mean / ratio_mean.max()\n",
    "    plot_data = torch.cat(\n",
    "        [\n",
    "            normalized_pca_mean,\n",
    "            normalized_twonn_mean,\n",
    "            normalized_ratio_mean,\n",
    "            normalized_std_mean,\n",
    "        ],\n",
    "        dim=1,\n",
    "    ).numpy()\n",
    "    annot_data = torch.cat([pca_mean, twonn_mean, ratio_mean, std_mean], dim=1).numpy()\n",
    "\n",
    "    im = ax[m].imshow(\n",
    "        plot_data,\n",
    "        aspect=\"auto\",\n",
    "        cmap=cmap,\n",
    "        origin=\"lower\",\n",
    "        vmin=0,\n",
    "        vmax=1,\n",
    "    )\n",
    "    for i in range(plot_data.shape[0]):\n",
    "        for j in range(plot_data.shape[1]):\n",
    "            value = annot_data[i, j]\n",
    "            map_value = plot_data[i, j]\n",
    "            color = (\n",
    "                \"black\" if (map_value < 0.8 and map_value > 0.3) else \"white\"\n",
    "            )  # Adjust text color based on cell color\n",
    "            ax[m].text(\n",
    "                j, i, f\"{value:.2f}\", ha=\"center\", va=\"center\", color=color\n",
    "            )  # , fontsize=14)\n",
    "    ax[m].set_title(model_names[model])  # , fontsize=14)\n",
    "    ax[m].set_xticks(np.arange(plot_data.shape[1]))\n",
    "    ax[m].set_xticklabels([\"PCA\", \"TwoNN\", \"Ratio\", \"Std\"])  # , fontsize=14)\n",
    "    if m == 0:\n",
    "        ax[m].set_yticks(np.arange(plot_data.shape[0]))\n",
    "        ax[m].set_yticklabels(np.arange(plot_data.shape[0]))  # , fontsize=14)\n",
    "        ax[m].set_ylabel(\"Layer\")  # , fontsize=14)\n",
    "    else:\n",
    "        ax[m].set_yticks([])\n",
    "\n",
    "plt.savefig(\"id_columns_99.pdf\", dpi=200, bbox_inches=\"tight\", format=\"pdf\")\n",
    "#!rsvg-convert -f pdf -o id_columns_99.pdf id_columns_99.svg\n",
    "#!rm id_columns_99.svg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\"openclip_l\", \"clip_l\", \"dinov2_l\", \"vit_l\", \"blip_l_flickr\", \"blip_l_coco\"]\n",
    "max_components = 64\n",
    "unit_evr = torch.zeros(len(models), 24, 16, max_components)\n",
    "\n",
    "for exp_path in experiments:\n",
    "    if \"imagenet\" not in exp_path.stem or \"_l\" not in exp_path.stem:\n",
    "        continue\n",
    "    exp_data = torch.load(exp_path, map_location=\"cpu\", weights_only=False)\n",
    "    for (layer_idx, head_idx, _unit_type), unit_data in exp_data[\"data\"].items():\n",
    "        model_idx = models.index(exp_data[\"model_name\"])\n",
    "        evr = unit_data[\"pca_evr\"][:max_components].unsqueeze(0)\n",
    "        new_evr = torch.cat([evr[:, 0].unsqueeze(-1), torch.diff(evr, dim=-1)], dim=-1)\n",
    "        unit_evr[model_idx, layer_idx, head_idx] = new_evr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_mean = torch.nn.functional.normalize(unit_evr[m].mean(dim=1), dim=-1, p=1)\n",
    "models = [\"openclip_l\", \"dinov2_l\"]\n",
    "plt.rcParams.update(figsizes.iclr2024(ncols=1, nrows=1, height_to_width_ratio=0.8))\n",
    "fig, ax = plt.subplots(nrows=1, ncols=len(models))\n",
    "k = 15\n",
    "for m, model in enumerate(models):\n",
    "    normalized_mean = torch.nn.functional.normalize(\n",
    "        unit_evr[m].mean(dim=1), dim=-1, p=1\n",
    "    )[:, :k]\n",
    "    im = ax[m].imshow(\n",
    "        normalized_mean,\n",
    "        aspect=\"auto\",\n",
    "        cmap=cmap,\n",
    "        origin=\"lower\",\n",
    "        # vmin=0,\n",
    "        # vmax=1,\n",
    "    )\n",
    "    ax[m].set_title(model_names[model])\n",
    "    ax[m].set_xticks(np.arange(normalized_mean.shape[1], step=2))\n",
    "    ax[m].set_xticklabels(np.arange(1, normalized_mean.shape[1] + 1, step=2))\n",
    "    if m == 1:\n",
    "        ax[m].yaxis.tick_right()\n",
    "        ax[m].yaxis.set_label_position(\"right\")\n",
    "        ax[m].set_yticks(np.arange(normalized_mean.shape[0]))\n",
    "        ax[m].set_yticklabels(np.arange(normalized_mean.shape[0]))\n",
    "        ax[m].set_ylabel(\"Layer\")\n",
    "    else:\n",
    "        ax[m].set_yticks([])\n",
    "    for i in range(normalized_mean.shape[0]):\n",
    "        row = normalized_mean[i]\n",
    "        cumsum = np.cumsum(row)\n",
    "        condition = cumsum >= 0.5\n",
    "        threshold_index = np.argmax(condition)\n",
    "        if (\n",
    "            threshold_index < k and condition.any()\n",
    "        ):  # Check if the threshold is within the plotted range\n",
    "            ax[m].axvline(\n",
    "                x=threshold_index + 0.4,\n",
    "                ymin=i / normalized_mean.shape[0],\n",
    "                ymax=(i + 1) / normalized_mean.shape[0],\n",
    "                color=\"y\",\n",
    "                linewidth=2,\n",
    "            )\n",
    "plt.savefig(\"evr_openclip_dino.svg\", dpi=200, bbox_inches=\"tight\", format=\"svg\")\n",
    "!rsvg-convert -f pdf -o evr_openclip_dino.pdf evr_openclip_dino.svg\n",
    "!rm evr_openclip_dino.svg"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
